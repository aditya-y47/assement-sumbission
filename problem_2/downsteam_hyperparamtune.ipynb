{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import Lime, LimeBase\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(self.data.iloc[index, :].values, dtype=torch.float32)\n",
    "        y = torch.tensor(self.targets.iloc[index], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "pth = \"./data/master_data_deberta.csv\"\n",
    "master_data = pd.read_csv(pth)\n",
    "master_data.drop(\"review\", axis=1, inplace=True)\n",
    "targets = master_data.pop(\"targets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    master_data, targets, test_size=0.3, stratify=targets\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.667, stratify=y_test)\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_feats: int = 1026) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_feats, 2**10)\n",
    "        self.fc2 = nn.Linear(2**10, 2**8)\n",
    "        self.fc3 = nn.Linear(2**8, 2**5)\n",
    "        self.fc4 = nn.Linear(2**5, 2**3)\n",
    "        self.fc5 = nn.Linear(2**3, 2**3)\n",
    "        self.final = nn.Linear(2**3, 1)\n",
    "        self.droup_out = nn.Dropout(0.33)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.selu(self.fc1(x))\n",
    "        x = F.selu(self.fc2(x))\n",
    "        x = self.droup_out(x)\n",
    "        x = F.selu(self.fc3(x))\n",
    "        x = self.droup_out(x)\n",
    "        x = F.selu(self.fc4(x))\n",
    "        x = F.selu(self.fc5(x))\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            prob = torch.sigmoid(logits)\n",
    "            pred = prob > 0.5\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the search space for each hyperparameter\n",
    "    epochs = trial.suggest_int(\"epochs\", 10, 30)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2)\n",
    "\n",
    "    # Create the model with the existing layer sizes and the hyperparameters from the search space\n",
    "    model = Network(input_feats=1026)\n",
    "    setattr(model, \"fc1\", nn.Linear(1026, 2**10))\n",
    "    setattr(model, \"fc2\", nn.Linear(2**10, 2**8))\n",
    "    setattr(model, \"fc3\", nn.Linear(2**8, 2**5))\n",
    "    setattr(model, \"fc4\", nn.Linear(2**5, 2**3))\n",
    "    setattr(model, \"fc5\", nn.Linear(2**3, 2**3))\n",
    "    setattr(model, \"final\", nn.Linear(2**3, 1))\n",
    "    setattr(model, \"droup_out\", nn.Dropout(dropout))\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = batch\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels.float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batch in val_dataloader:\n",
    "                inputs, labels = batch\n",
    "                logits = model(inputs)\n",
    "                val_loss += criterion(logits, labels.float().unsqueeze(1)).item()\n",
    "            val_loss /= len(val_dataloader)\n",
    "\n",
    "        # Update the Optuna trial with the validation loss\n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        # Prune the trial if the validation loss is too high\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Return the validation loss as the objective value\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-12 23:55:42,140]\u001b[0m A new study created in memory with name: no-name-c89ac3cd-7563-47a4-a0e8-cea3523b7e11\u001b[0m\n",
      "\u001b[32m[I 2023-03-12 23:59:46,652]\u001b[0m Trial 0 finished with value: 0.28901556112310467 and parameters: {'epochs': 24, 'dropout': 0.41170711473010735, 'learning_rate': 0.00019937232653442498}. Best is trial 0 with value: 0.28901556112310467.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:02:30,876]\u001b[0m Trial 1 finished with value: 0.6692610632847337 and parameters: {'epochs': 14, 'dropout': 0.14839005241159395, 'learning_rate': 0.009919117726188578}. Best is trial 0 with value: 0.28901556112310467.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:07:04,504]\u001b[0m Trial 2 finished with value: 0.6690779735936838 and parameters: {'epochs': 24, 'dropout': 0.38502507076029335, 'learning_rate': 0.008394919051794897}. Best is trial 0 with value: 0.28901556112310467.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:10:55,974]\u001b[0m Trial 3 finished with value: 0.6698261183850905 and parameters: {'epochs': 19, 'dropout': 0.31272308123605935, 'learning_rate': 0.007399637351680926}. Best is trial 0 with value: 0.28901556112310467.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:14:02,111]\u001b[0m Trial 4 finished with value: 0.6702094498802634 and parameters: {'epochs': 18, 'dropout': 0.32437995823251914, 'learning_rate': 0.006931897390597142}. Best is trial 0 with value: 0.28901556112310467.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:16:21,892]\u001b[0m Trial 5 finished with value: 0.2999749260351938 and parameters: {'epochs': 16, 'dropout': 0.19622592157793878, 'learning_rate': 0.0004221380289498234}. Best is trial 0 with value: 0.28901556112310467.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:19:40,143]\u001b[0m Trial 6 finished with value: 0.2870278911774649 and parameters: {'epochs': 22, 'dropout': 0.21148331846092258, 'learning_rate': 0.0010051668350152649}. Best is trial 6 with value: 0.2870278911774649.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:19:50,228]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:23:01,905]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:23:15,395]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:26:35,678]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:30:24,064]\u001b[0m Trial 11 finished with value: 0.2968554807936444 and parameters: {'epochs': 26, 'dropout': 0.4948934401965207, 'learning_rate': 6.919061407400975e-05}. Best is trial 6 with value: 0.2870278911774649.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:33:21,244]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:33:29,932]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:33:38,845]\u001b[0m Trial 14 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:37:45,826]\u001b[0m Trial 15 finished with value: 0.2808081199798514 and parameters: {'epochs': 28, 'dropout': 0.11335948042600288, 'learning_rate': 6.970003349887182e-05}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:38:03,119]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:38:20,566]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:38:38,320]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:38:47,816]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:39:05,399]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:39:22,819]\u001b[0m Trial 21 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:39:39,996]\u001b[0m Trial 22 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:42:54,839]\u001b[0m Trial 23 finished with value: 0.291118071798016 and parameters: {'epochs': 22, 'dropout': 0.2901802727820473, 'learning_rate': 0.0009245312932131707}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:43:12,441]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:43:21,416]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:47:30,049]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:47:47,584]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:48:05,463]\u001b[0m Trial 28 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:48:23,248]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:48:31,945]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:48:49,353]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:48:58,057]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:49:06,734]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:49:15,610]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:49:33,283]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:49:50,613]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:50:08,302]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:50:25,928]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:50:34,659]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:50:43,651]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:54:33,926]\u001b[0m Trial 41 finished with value: 0.32454291670857105 and parameters: {'epochs': 26, 'dropout': 0.4961566828574414, 'learning_rate': 0.00011150806266322738}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:54:42,819]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:56:28,945]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:56:37,590]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:56:46,415]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:56:55,346]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:57:03,911]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:57:13,334]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:57:31,793]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:57:40,590]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 00:57:49,332]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:00:09,966]\u001b[0m Trial 52 finished with value: 0.29594191799269004 and parameters: {'epochs': 16, 'dropout': 0.18920743883680607, 'learning_rate': 0.0004435210237850565}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:00:18,817]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:00:44,844]\u001b[0m Trial 54 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:01:02,352]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:01:11,114]\u001b[0m Trial 56 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:01:19,801]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:01:37,378]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:01:54,825]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:04:23,140]\u001b[0m Trial 60 finished with value: 0.3252546189572005 and parameters: {'epochs': 17, 'dropout': 0.1737641786334171, 'learning_rate': 0.0003206465499055282}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:04:32,116]\u001b[0m Trial 61 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:04:49,863]\u001b[0m Trial 62 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:06:35,385]\u001b[0m Trial 63 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:06:44,143]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:07:01,708]\u001b[0m Trial 65 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:07:19,253]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:07:28,036]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:07:36,812]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:09:57,459]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:11:35,745]\u001b[0m Trial 70 finished with value: 0.33956491629428726 and parameters: {'epochs': 11, 'dropout': 0.2817414124522938, 'learning_rate': 0.0015184683186703395}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:11:44,618]\u001b[0m Trial 71 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:12:02,102]\u001b[0m Trial 72 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:12:10,783]\u001b[0m Trial 73 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:12:19,440]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:15:50,340]\u001b[0m Trial 75 finished with value: 0.40413738611866445 and parameters: {'epochs': 24, 'dropout': 0.22468855467556095, 'learning_rate': 0.0005229398927616054}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:15:59,157]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:16:08,004]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:16:16,877]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:16:34,682]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:16:52,366]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:19:21,064]\u001b[0m Trial 81 finished with value: 0.31674568924833746 and parameters: {'epochs': 17, 'dropout': 0.1770897279091091, 'learning_rate': 0.00031984648466213805}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:19:29,943]\u001b[0m Trial 82 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:19:47,339]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:19:56,013]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:20:04,824]\u001b[0m Trial 85 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:20:31,208]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:20:39,954]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:20:48,729]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:20:57,618]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:23:09,814]\u001b[0m Trial 90 finished with value: 0.2946148199193618 and parameters: {'epochs': 15, 'dropout': 0.1303055221591474, 'learning_rate': 0.0010430509232650898}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:24:55,729]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:26:43,203]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:27:00,681]\u001b[0m Trial 93 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:27:09,570]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:27:52,933]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:28:10,365]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:28:19,231]\u001b[0m Trial 97 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:30:13,368]\u001b[0m Trial 98 finished with value: 0.29426480073700934 and parameters: {'epochs': 13, 'dropout': 0.40070535790950856, 'learning_rate': 0.0004478491078161763}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:30:22,170]\u001b[0m Trial 99 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:30:31,022]\u001b[0m Trial 100 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:30:48,694]\u001b[0m Trial 101 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:30:57,473]\u001b[0m Trial 102 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:31:06,338]\u001b[0m Trial 103 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:31:15,115]\u001b[0m Trial 104 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:31:23,928]\u001b[0m Trial 105 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:31:41,654]\u001b[0m Trial 106 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:31:50,424]\u001b[0m Trial 107 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:31:59,143]\u001b[0m Trial 108 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:32:16,892]\u001b[0m Trial 109 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:33:09,036]\u001b[0m Trial 110 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:33:17,849]\u001b[0m Trial 111 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:35:03,305]\u001b[0m Trial 112 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:35:20,786]\u001b[0m Trial 113 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:35:38,335]\u001b[0m Trial 114 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:35:47,181]\u001b[0m Trial 115 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:36:04,544]\u001b[0m Trial 116 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:36:21,683]\u001b[0m Trial 117 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:36:30,687]\u001b[0m Trial 118 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:38:25,708]\u001b[0m Trial 119 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:38:34,599]\u001b[0m Trial 120 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:38:43,470]\u001b[0m Trial 121 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:39:27,269]\u001b[0m Trial 122 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:39:36,149]\u001b[0m Trial 123 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:39:44,958]\u001b[0m Trial 124 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:42:06,645]\u001b[0m Trial 125 finished with value: 0.29275502461720915 and parameters: {'epochs': 16, 'dropout': 0.2817947781701533, 'learning_rate': 0.0003615418442186064}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:42:15,588]\u001b[0m Trial 126 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:44:35,787]\u001b[0m Trial 127 finished with value: 0.29847587612183657 and parameters: {'epochs': 16, 'dropout': 0.29263978805938157, 'learning_rate': 0.00042269242845721183}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:44:44,366]\u001b[0m Trial 128 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:44:53,015]\u001b[0m Trial 129 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:45:01,661]\u001b[0m Trial 130 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:45:10,295]\u001b[0m Trial 131 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:45:27,584]\u001b[0m Trial 132 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:45:36,257]\u001b[0m Trial 133 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:45:53,517]\u001b[0m Trial 134 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:46:02,146]\u001b[0m Trial 135 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:46:19,699]\u001b[0m Trial 136 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:46:37,223]\u001b[0m Trial 137 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:46:46,005]\u001b[0m Trial 138 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:46:54,814]\u001b[0m Trial 139 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:47:05,626]\u001b[0m Trial 140 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:47:14,273]\u001b[0m Trial 141 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:47:31,563]\u001b[0m Trial 142 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:49:43,462]\u001b[0m Trial 143 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:49:52,148]\u001b[0m Trial 144 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:51:45,485]\u001b[0m Trial 145 finished with value: 0.3194139139617191 and parameters: {'epochs': 13, 'dropout': 0.19498775726935538, 'learning_rate': 0.00042274124208442447}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:53:40,741]\u001b[0m Trial 146 finished with value: 0.30219212700338927 and parameters: {'epochs': 13, 'dropout': 0.19673716545204098, 'learning_rate': 0.0003633277022183447}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:53:49,700]\u001b[0m Trial 147 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:53:58,400]\u001b[0m Trial 148 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:54:07,136]\u001b[0m Trial 149 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:54:15,785]\u001b[0m Trial 150 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:54:25,149]\u001b[0m Trial 151 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:56:09,330]\u001b[0m Trial 152 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:57:02,101]\u001b[0m Trial 153 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:57:10,926]\u001b[0m Trial 154 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:57:20,333]\u001b[0m Trial 155 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:57:29,227]\u001b[0m Trial 156 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:57:47,051]\u001b[0m Trial 157 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:57:56,001]\u001b[0m Trial 158 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:58:13,667]\u001b[0m Trial 159 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:58:31,390]\u001b[0m Trial 160 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:58:49,113]\u001b[0m Trial 161 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:58:58,022]\u001b[0m Trial 162 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:59:06,533]\u001b[0m Trial 163 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:59:15,146]\u001b[0m Trial 164 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:59:23,724]\u001b[0m Trial 165 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:59:32,420]\u001b[0m Trial 166 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:59:41,379]\u001b[0m Trial 167 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 01:59:59,306]\u001b[0m Trial 168 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:00:08,174]\u001b[0m Trial 169 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:01:01,481]\u001b[0m Trial 170 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:04:32,594]\u001b[0m Trial 171 finished with value: 0.2904505251961596 and parameters: {'epochs': 24, 'dropout': 0.22708771559819602, 'learning_rate': 0.0005390755673282442}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:04:41,650]\u001b[0m Trial 172 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:04:59,327]\u001b[0m Trial 173 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:05:08,172]\u001b[0m Trial 174 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:05:17,294]\u001b[0m Trial 175 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:05:26,116]\u001b[0m Trial 176 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:05:34,972]\u001b[0m Trial 177 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:05:52,694]\u001b[0m Trial 178 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:06:01,936]\u001b[0m Trial 179 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:06:10,556]\u001b[0m Trial 180 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:06:19,172]\u001b[0m Trial 181 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:06:36,345]\u001b[0m Trial 182 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:06:45,019]\u001b[0m Trial 183 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:06:54,169]\u001b[0m Trial 184 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:07:11,496]\u001b[0m Trial 185 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:07:28,767]\u001b[0m Trial 186 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:07:46,107]\u001b[0m Trial 187 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:07:54,874]\u001b[0m Trial 188 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:08:12,404]\u001b[0m Trial 189 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:09:56,389]\u001b[0m Trial 190 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:10:05,457]\u001b[0m Trial 191 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:10:16,972]\u001b[0m Trial 192 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:10:26,687]\u001b[0m Trial 193 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:10:36,151]\u001b[0m Trial 194 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:10:45,044]\u001b[0m Trial 195 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:10:54,463]\u001b[0m Trial 196 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:11:04,102]\u001b[0m Trial 197 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:11:12,922]\u001b[0m Trial 198 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:11:21,661]\u001b[0m Trial 199 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:12:14,547]\u001b[0m Trial 200 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:12:27,502]\u001b[0m Trial 201 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:12:37,241]\u001b[0m Trial 202 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:12:49,638]\u001b[0m Trial 203 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:13:03,631]\u001b[0m Trial 204 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:13:13,756]\u001b[0m Trial 205 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:14:05,349]\u001b[0m Trial 206 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:14:13,995]\u001b[0m Trial 207 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:14:25,301]\u001b[0m Trial 208 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:14:34,489]\u001b[0m Trial 209 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:14:51,885]\u001b[0m Trial 210 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:15:02,435]\u001b[0m Trial 211 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:15:12,387]\u001b[0m Trial 212 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:15:24,135]\u001b[0m Trial 213 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:15:35,177]\u001b[0m Trial 214 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:15:46,717]\u001b[0m Trial 215 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:15:56,004]\u001b[0m Trial 216 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:16:04,813]\u001b[0m Trial 217 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:16:15,743]\u001b[0m Trial 218 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:16:28,856]\u001b[0m Trial 219 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:16:37,592]\u001b[0m Trial 220 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:16:46,417]\u001b[0m Trial 221 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:16:56,163]\u001b[0m Trial 222 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:17:04,935]\u001b[0m Trial 223 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:17:13,698]\u001b[0m Trial 224 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:17:23,601]\u001b[0m Trial 225 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:17:34,915]\u001b[0m Trial 226 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:17:52,629]\u001b[0m Trial 227 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:18:01,521]\u001b[0m Trial 228 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:18:10,370]\u001b[0m Trial 229 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:18:25,980]\u001b[0m Trial 230 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:18:43,545]\u001b[0m Trial 231 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:18:53,464]\u001b[0m Trial 232 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:19:03,721]\u001b[0m Trial 233 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:19:13,342]\u001b[0m Trial 234 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:19:22,994]\u001b[0m Trial 235 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:19:31,769]\u001b[0m Trial 236 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:19:40,622]\u001b[0m Trial 237 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:19:49,462]\u001b[0m Trial 238 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:19:59,450]\u001b[0m Trial 239 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:20:08,197]\u001b[0m Trial 240 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:20:25,795]\u001b[0m Trial 241 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:20:34,623]\u001b[0m Trial 242 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:20:44,227]\u001b[0m Trial 243 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:20:54,415]\u001b[0m Trial 244 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:21:03,143]\u001b[0m Trial 245 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:21:11,923]\u001b[0m Trial 246 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:21:25,049]\u001b[0m Trial 247 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:21:33,839]\u001b[0m Trial 248 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:21:42,510]\u001b[0m Trial 249 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:21:51,170]\u001b[0m Trial 250 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:22:00,309]\u001b[0m Trial 251 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:22:09,011]\u001b[0m Trial 252 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:22:17,671]\u001b[0m Trial 253 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:22:26,374]\u001b[0m Trial 254 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:22:35,014]\u001b[0m Trial 255 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:22:43,696]\u001b[0m Trial 256 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:22:53,493]\u001b[0m Trial 257 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:23:09,480]\u001b[0m Trial 258 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:23:21,035]\u001b[0m Trial 259 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:23:29,707]\u001b[0m Trial 260 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:23:39,102]\u001b[0m Trial 261 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:23:48,897]\u001b[0m Trial 262 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:24:02,607]\u001b[0m Trial 263 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:24:12,058]\u001b[0m Trial 264 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:24:20,764]\u001b[0m Trial 265 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:24:31,368]\u001b[0m Trial 266 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:24:41,243]\u001b[0m Trial 267 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:24:51,519]\u001b[0m Trial 268 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:26:46,331]\u001b[0m Trial 269 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:26:55,380]\u001b[0m Trial 270 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:27:04,852]\u001b[0m Trial 271 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:27:13,660]\u001b[0m Trial 272 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:27:25,631]\u001b[0m Trial 273 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:27:34,279]\u001b[0m Trial 274 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:27:44,975]\u001b[0m Trial 275 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:27:53,691]\u001b[0m Trial 276 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:28:02,420]\u001b[0m Trial 277 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:28:12,002]\u001b[0m Trial 278 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:28:20,968]\u001b[0m Trial 279 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:28:31,367]\u001b[0m Trial 280 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:28:40,550]\u001b[0m Trial 281 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:28:58,137]\u001b[0m Trial 282 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:29:06,864]\u001b[0m Trial 283 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:29:15,847]\u001b[0m Trial 284 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:29:26,523]\u001b[0m Trial 285 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:29:39,448]\u001b[0m Trial 286 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:29:52,873]\u001b[0m Trial 287 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:30:10,496]\u001b[0m Trial 288 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:30:19,307]\u001b[0m Trial 289 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:30:33,968]\u001b[0m Trial 290 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:30:42,797]\u001b[0m Trial 291 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:30:51,686]\u001b[0m Trial 292 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:31:02,966]\u001b[0m Trial 293 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:31:11,720]\u001b[0m Trial 294 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:31:20,643]\u001b[0m Trial 295 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:31:38,206]\u001b[0m Trial 296 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:31:47,026]\u001b[0m Trial 297 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:31:57,715]\u001b[0m Trial 298 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:32:07,889]\u001b[0m Trial 299 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:32:16,697]\u001b[0m Trial 300 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:32:30,881]\u001b[0m Trial 301 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:32:41,627]\u001b[0m Trial 302 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:32:52,940]\u001b[0m Trial 303 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:33:01,667]\u001b[0m Trial 304 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:33:10,416]\u001b[0m Trial 305 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:33:27,794]\u001b[0m Trial 306 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:33:37,878]\u001b[0m Trial 307 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:33:48,439]\u001b[0m Trial 308 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:33:57,115]\u001b[0m Trial 309 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:34:08,792]\u001b[0m Trial 310 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:34:43,694]\u001b[0m Trial 311 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:34:53,425]\u001b[0m Trial 312 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:35:03,281]\u001b[0m Trial 313 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:35:20,794]\u001b[0m Trial 314 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:35:29,619]\u001b[0m Trial 315 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:35:38,378]\u001b[0m Trial 316 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:35:55,885]\u001b[0m Trial 317 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:36:10,048]\u001b[0m Trial 318 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:36:20,306]\u001b[0m Trial 319 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:36:36,822]\u001b[0m Trial 320 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:36:45,789]\u001b[0m Trial 321 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:36:54,400]\u001b[0m Trial 322 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:37:04,030]\u001b[0m Trial 323 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:37:12,659]\u001b[0m Trial 324 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:37:23,819]\u001b[0m Trial 325 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:37:32,433]\u001b[0m Trial 326 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:37:41,841]\u001b[0m Trial 327 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:37:53,443]\u001b[0m Trial 328 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:38:02,491]\u001b[0m Trial 329 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:38:16,122]\u001b[0m Trial 330 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:38:33,918]\u001b[0m Trial 331 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:38:42,745]\u001b[0m Trial 332 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:38:51,760]\u001b[0m Trial 333 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:39:00,672]\u001b[0m Trial 334 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:39:09,909]\u001b[0m Trial 335 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:39:20,798]\u001b[0m Trial 336 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:39:31,032]\u001b[0m Trial 337 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:39:39,917]\u001b[0m Trial 338 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:39:49,003]\u001b[0m Trial 339 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:39:58,313]\u001b[0m Trial 340 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:40:07,455]\u001b[0m Trial 341 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:40:16,264]\u001b[0m Trial 342 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:40:29,550]\u001b[0m Trial 343 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:40:47,308]\u001b[0m Trial 344 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:40:56,423]\u001b[0m Trial 345 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:41:05,289]\u001b[0m Trial 346 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:41:16,712]\u001b[0m Trial 347 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:41:25,556]\u001b[0m Trial 348 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:41:34,250]\u001b[0m Trial 349 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:41:51,944]\u001b[0m Trial 350 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:42:04,750]\u001b[0m Trial 351 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:42:13,365]\u001b[0m Trial 352 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:42:31,404]\u001b[0m Trial 353 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:42:45,038]\u001b[0m Trial 354 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:42:53,948]\u001b[0m Trial 355 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:43:11,657]\u001b[0m Trial 356 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:44:04,475]\u001b[0m Trial 357 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:44:13,332]\u001b[0m Trial 358 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:45:56,946]\u001b[0m Trial 359 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:46:05,978]\u001b[0m Trial 360 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:46:15,002]\u001b[0m Trial 361 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:46:26,884]\u001b[0m Trial 362 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:48:38,087]\u001b[0m Trial 363 finished with value: 0.3040283718968139 and parameters: {'epochs': 15, 'dropout': 0.22728228866687228, 'learning_rate': 0.00016354364050346075}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:48:55,352]\u001b[0m Trial 364 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:49:06,162]\u001b[0m Trial 365 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:49:14,973]\u001b[0m Trial 366 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:49:32,219]\u001b[0m Trial 367 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:49:43,374]\u001b[0m Trial 368 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:49:53,518]\u001b[0m Trial 369 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:50:03,453]\u001b[0m Trial 370 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:50:12,234]\u001b[0m Trial 371 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:50:24,018]\u001b[0m Trial 372 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:50:32,858]\u001b[0m Trial 373 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:50:43,179]\u001b[0m Trial 374 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:50:55,872]\u001b[0m Trial 375 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:51:05,200]\u001b[0m Trial 376 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:51:22,910]\u001b[0m Trial 377 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:51:34,057]\u001b[0m Trial 378 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:51:46,961]\u001b[0m Trial 379 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:51:55,817]\u001b[0m Trial 380 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:52:05,016]\u001b[0m Trial 381 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:52:15,142]\u001b[0m Trial 382 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:52:23,932]\u001b[0m Trial 383 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:52:36,074]\u001b[0m Trial 384 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:52:47,197]\u001b[0m Trial 385 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:53:04,808]\u001b[0m Trial 386 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:53:15,520]\u001b[0m Trial 387 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:53:24,323]\u001b[0m Trial 388 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:53:37,592]\u001b[0m Trial 389 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:53:46,392]\u001b[0m Trial 390 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:53:55,232]\u001b[0m Trial 391 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:54:05,892]\u001b[0m Trial 392 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:54:16,358]\u001b[0m Trial 393 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:54:26,723]\u001b[0m Trial 394 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:54:35,507]\u001b[0m Trial 395 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:54:51,591]\u001b[0m Trial 396 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:55:00,383]\u001b[0m Trial 397 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:55:09,490]\u001b[0m Trial 398 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:55:20,459]\u001b[0m Trial 399 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:55:29,205]\u001b[0m Trial 400 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:55:37,842]\u001b[0m Trial 401 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:55:46,540]\u001b[0m Trial 402 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:55:56,078]\u001b[0m Trial 403 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:59:37,983]\u001b[0m Trial 404 finished with value: 0.29207342868561254 and parameters: {'epochs': 25, 'dropout': 0.23284994259053934, 'learning_rate': 0.00040828626354378337}. Best is trial 15 with value: 0.2808081199798514.\u001b[0m\n",
      "\u001b[32m[I 2023-03-13 02:59:55,616]\u001b[0m Trial 405 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:00:04,431]\u001b[0m Trial 406 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:00:15,541]\u001b[0m Trial 407 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:00:24,396]\u001b[0m Trial 408 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:00:34,201]\u001b[0m Trial 409 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:00:42,845]\u001b[0m Trial 410 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:00:52,836]\u001b[0m Trial 411 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:01:35,993]\u001b[0m Trial 412 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:01:53,344]\u001b[0m Trial 413 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:02:03,495]\u001b[0m Trial 414 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:02:15,102]\u001b[0m Trial 415 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:02:25,935]\u001b[0m Trial 416 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:02:36,019]\u001b[0m Trial 417 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:02:47,313]\u001b[0m Trial 418 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:03:30,355]\u001b[0m Trial 419 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:03:41,392]\u001b[0m Trial 420 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:03:50,056]\u001b[0m Trial 421 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:04:00,231]\u001b[0m Trial 422 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:04:09,279]\u001b[0m Trial 423 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:04:18,128]\u001b[0m Trial 424 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:04:27,082]\u001b[0m Trial 425 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:04:41,568]\u001b[0m Trial 426 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:04:50,959]\u001b[0m Trial 427 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:05:03,348]\u001b[0m Trial 428 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:05:12,252]\u001b[0m Trial 429 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:05:22,288]\u001b[0m Trial 430 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:05:31,090]\u001b[0m Trial 431 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:05:39,994]\u001b[0m Trial 432 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:05:52,662]\u001b[0m Trial 433 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:06:01,429]\u001b[0m Trial 434 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:06:44,920]\u001b[0m Trial 435 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:06:59,012]\u001b[0m Trial 436 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:07:08,245]\u001b[0m Trial 437 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:07:20,176]\u001b[0m Trial 438 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:07:37,671]\u001b[0m Trial 439 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:08:29,859]\u001b[0m Trial 440 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:08:38,573]\u001b[0m Trial 441 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:08:47,377]\u001b[0m Trial 442 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:08:56,023]\u001b[0m Trial 443 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:09:13,294]\u001b[0m Trial 444 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:09:22,087]\u001b[0m Trial 445 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:09:32,159]\u001b[0m Trial 446 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:09:43,395]\u001b[0m Trial 447 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:09:52,504]\u001b[0m Trial 448 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:10:03,090]\u001b[0m Trial 449 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:10:11,841]\u001b[0m Trial 450 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:10:20,777]\u001b[0m Trial 451 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:10:30,088]\u001b[0m Trial 452 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:10:44,589]\u001b[0m Trial 453 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:11:02,038]\u001b[0m Trial 454 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:11:14,191]\u001b[0m Trial 455 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:11:23,185]\u001b[0m Trial 456 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:11:34,891]\u001b[0m Trial 457 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:11:46,706]\u001b[0m Trial 458 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:11:55,575]\u001b[0m Trial 459 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:12:05,806]\u001b[0m Trial 460 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:12:14,618]\u001b[0m Trial 461 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:12:32,419]\u001b[0m Trial 462 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:12:50,158]\u001b[0m Trial 463 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:13:00,591]\u001b[0m Trial 464 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:13:09,473]\u001b[0m Trial 465 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:13:18,350]\u001b[0m Trial 466 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:13:27,325]\u001b[0m Trial 467 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:13:45,279]\u001b[0m Trial 468 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:14:01,597]\u001b[0m Trial 469 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:14:10,321]\u001b[0m Trial 470 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:14:23,338]\u001b[0m Trial 471 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:14:40,853]\u001b[0m Trial 472 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:14:49,609]\u001b[0m Trial 473 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:14:58,594]\u001b[0m Trial 474 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:15:16,344]\u001b[0m Trial 475 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:15:28,354]\u001b[0m Trial 476 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:15:39,205]\u001b[0m Trial 477 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:15:48,128]\u001b[0m Trial 478 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:15:56,747]\u001b[0m Trial 479 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:16:06,818]\u001b[0m Trial 480 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:16:15,542]\u001b[0m Trial 481 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:16:24,257]\u001b[0m Trial 482 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:16:41,906]\u001b[0m Trial 483 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:16:51,944]\u001b[0m Trial 484 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:17:02,826]\u001b[0m Trial 485 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:17:12,744]\u001b[0m Trial 486 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:17:23,575]\u001b[0m Trial 487 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:17:34,316]\u001b[0m Trial 488 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:17:46,962]\u001b[0m Trial 489 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:17:57,338]\u001b[0m Trial 490 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:18:06,314]\u001b[0m Trial 491 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:18:15,143]\u001b[0m Trial 492 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:18:23,933]\u001b[0m Trial 493 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:18:34,520]\u001b[0m Trial 494 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:18:43,429]\u001b[0m Trial 495 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:18:52,414]\u001b[0m Trial 496 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:19:02,470]\u001b[0m Trial 497 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:19:20,176]\u001b[0m Trial 498 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-13 03:19:33,574]\u001b[0m Trial 499 pruned. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# # Set the number of epochs and the number of trials for Optuna\n",
    "# n_trials = 500\n",
    "\n",
    "# # Create an Optuna study and run the optimization\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "# # Get the best hyperparameters from the study\n",
    "# best_params = study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 28,\n",
       " 'dropout': 0.11335948042600288,\n",
       " 'learning_rate': 6.970003349887182e-05}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_trials = study.trials_dataframe().nlargest(10, \"value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial #299:\n",
      "Objective value = 0.8868\n",
      "number                                         299\n",
      "value                                     0.886823\n",
      "datetime_start          2023-03-13 02:31:57.716394\n",
      "datetime_complete       2023-03-13 02:32:07.889162\n",
      "duration                    0 days 00:00:10.172768\n",
      "params_dropout                            0.286248\n",
      "params_epochs                                   18\n",
      "params_learning_rate                      0.008898\n",
      "state                                       PRUNED\n",
      "Name: 299, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the trial number, objective value, and parameter values for the top 10 trials\n",
    "for i, trial in top_10_trials.iterrows():\n",
    "    print(f\"Trial #{trial.number}:\")\n",
    "    print(f\"Objective value = {trial.value:.4f}\")\n",
    "    print((trial))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
