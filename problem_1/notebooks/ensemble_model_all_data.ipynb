{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"./clean_data.csv\")\n",
    "targets = data.pop(list(data.columns)[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, targets, test_size=0.3, random_state=121, stratify=targets\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_test, y_test, test_size=0.67, random_state=121, stratify=y_test\n",
    ")\n",
    "data = {}\n",
    "data[\"train\"] = [X_train, y_train]\n",
    "data[\"val\"] = [X_val, y_val]\n",
    "data[\"test\"] = [X_test, y_test]\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "class_weights = {}\n",
    "total_samples = np.sum(counts)\n",
    "for cls in class_counts:\n",
    "    class_weights[cls] = total_samples / (len(class_counts) * class_counts[cls])\n",
    "sample_weights = np.array([class_weights[cls] for cls in y_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.4168126094570928,\n",
       " 'recall': 0.7555555555555555,\n",
       " 'f1_score': 0.5372460496613995,\n",
       " 'auc_roc': 0.954606079602066,\n",
       " 'auc_pr': 0.7111286389430058}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics_sklearn(X, y, clf):\n",
    "    y_pred = clf.predict(X)\n",
    "    y_pred_prob = clf.predict_proba(X)[:, 1]\n",
    "\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    auc_roc = roc_auc_score(y, y_pred_prob)\n",
    "    auc_pr = average_precision_score(y, y_pred_prob)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"auc_roc\": auc_roc,\n",
    "        \"auc_pr\": auc_pr,\n",
    "    }\n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "# Testing a basic GBDT as a baseline\n",
    "clf_1 = HistGradientBoostingClassifier()\n",
    "\n",
    "clf_1.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "compute_metrics_sklearn(X=X_test, y=y_test, clf=clf_1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meat of the stuff here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-aucpr:0.54374\tvalidation-aucpr:0.51460\n",
      "[9]\ttrain-aucpr:0.85679\tvalidation-aucpr:0.66455\n",
      "{'Training Subset': \"{'precision': 0.43474646716541976, 'recall': \"\n",
      "                    \"0.9526411657559198, 'f1_score': 0.5970319634703195, \"\n",
      "                    \"'auc_roc': 0.9874339699113004, 'auc_pr': \"\n",
      "                    \"0.870033272836601, 'threshold': 0.5278838}\"}\n",
      "{'Validation Subset': \"{'precision': 0.33620689655172414, 'recall': \"\n",
      "                      \"0.7548387096774194, 'f1_score': 0.46520874751491054, \"\n",
      "                      \"'auc_roc': 0.9405523943750855, 'auc_pr': \"\n",
      "                      \"0.6720859202648976, 'threshold': 0.5278838}\"}\n"
     ]
    }
   ],
   "source": [
    "from predictors import XGBoostClassifier, LightGBMClassifier, WeightedEnsembleClassifier\n",
    "from custom_voter_boost_all_data import all_estimators, compute_metrics\n",
    "\n",
    "xgb_1 = XGBoostClassifier(data=data)\n",
    "xgb_1.fit(None, None)\n",
    "\n",
    "pprint({\"Training Subset\": f\"{xgb_1.score(X=data['train'][0],y=data['train'][1])}\"})\n",
    "pprint({\"Validation Subset\": f\"{xgb_1.score(X=data['val'][0],y=data['val'][1])}\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1098, number of negative: 21689\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16320\n",
      "[LightGBM] [Info] Number of data points in the train set: 22787, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.048185 -> initscore=-2.983315\n",
      "[LightGBM] [Info] Start training from score -2.983315\n",
      "{'Training Subset': \"{'precision': 0.43474646716541976, 'recall': \"\n",
      "                    \"0.9526411657559198, 'f1_score': 0.5970319634703195, \"\n",
      "                    \"'auc_roc': 0.9874339699113004, 'auc_pr': \"\n",
      "                    \"0.870033272836601, 'threshold': 0.5278838}\"}\n",
      "{'Validation Subset': \"{'precision': 0.33620689655172414, 'recall': \"\n",
      "                      \"0.7548387096774194, 'f1_score': 0.46520874751491054, \"\n",
      "                      \"'auc_roc': 0.9405523943750855, 'auc_pr': \"\n",
      "                      \"0.6720859202648976, 'threshold': 0.5278838}\"}\n"
     ]
    }
   ],
   "source": [
    "lgbm_1 = LightGBMClassifier(data=data)\n",
    "lgbm_1.fit(None, None)\n",
    "pprint({\"Training Subset\": f\"{xgb_1.score(X=data['train'][0],y=data['train'][1])}\"})\n",
    "pprint({\"Validation Subset\": f\"{xgb_1.score(X=data['val'][0],y=data['val'][1])}\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_score': 0.6894075403949731,\n",
      " 'precision': 0.7933884297520661,\n",
      " 'recall': 0.6095238095238096}\n"
     ]
    }
   ],
   "source": [
    "voter = WeightedEnsembleClassifier(estimators=all_estimators, weights=None)\n",
    "wts, mean_threshold = voter.score(data[\"val\"][0], data[\"val\"][1])\n",
    "voter_2 = WeightedEnsembleClassifier(estimators=all_estimators, weights=wts)\n",
    "preds = voter_2.predict(X=data[\"test\"][0], threshold=mean_threshold)\n",
    "pprint(compute_metrics(y_pred=preds, y_test=data[\"test\"][1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
